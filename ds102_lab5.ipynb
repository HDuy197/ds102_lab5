{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HDuy197/ds102_lab5/blob/main/ds102_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DfhmYFA5OOo_",
      "metadata": {
        "id": "DfhmYFA5OOo_"
      },
      "source": [
        "## Bài 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3EU0UmurvRM7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EU0UmurvRM7",
        "outputId": "fed6a99f-0c6e-48c0-841d-a74bbf99a90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py_vncorenlp in /usr/local/lib/python3.11/dist-packages (0.1.4)\n",
            "Requirement already satisfied: pyjnius in /usr/local/lib/python3.11/dist-packages (from py_vncorenlp) (1.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install py_vncorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "tyfVXxuGvRM8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyfVXxuGvRM8",
        "outputId": "cc20571d-8d0c-444c-e3ca-66ee4fea9668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VnCoreNLP model folder /content/vncorenlp already exists! Please load VnCoreNLP from this folder!\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/vncorenlp\n",
        "\n",
        "import py_vncorenlp\n",
        "\n",
        "py_vncorenlp.download_model(save_dir='/content/vncorenlp')\n",
        "model_vncorenlp =py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"],\n",
        "                            save_dir=\"/content/vncorenlp\",\n",
        "                            max_heap_size='-Xmx2g')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Xt9iZx6-vRM9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt9iZx6-vRM9",
        "outputId": "a4400373-4153-4ddb-aaa7-76bd6d47118a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eJdEStCQBf3GtbOIQfIR2iS-Wb3nlfJX\n",
            "To: /content/uit_vsfc_data/train.json\n",
            "100%|██████████| 2.02M/2.02M [00:00<00:00, 91.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UWbzHFeUPu4AuofOEy0Fo0SFotVZHzUe\n",
            "To: /content/uit_vsfc_data/dev.json\n",
            "100%|██████████| 274k/274k [00:00<00:00, 46.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g4imdabNbswIu91K6zKFGalkjDBBqaQq\n",
            "To: /content/uit_vsfc_data/test.json\n",
            "100%|██████████| 559k/559k [00:00<00:00, 38.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import os\n",
        "import json\n",
        "\n",
        "data_dir = '/content/uit_vsfc_data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "file_ids = {\n",
        "    \"train.json\": \"1eJdEStCQBf3GtbOIQfIR2iS-Wb3nlfJX\",\n",
        "    \"dev.json\": \"1UWbzHFeUPu4AuofOEy0Fo0SFotVZHzUe\",\n",
        "    \"test.json\": \"1g4imdabNbswIu91K6zKFGalkjDBBqaQq\"\n",
        "}\n",
        "\n",
        "for filename, file_id in file_ids.items():\n",
        "    output_path = os.path.join(data_dir, filename)\n",
        "    gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "\n",
        "def load_data(filename):\n",
        "    with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_data = load_data('train.json')\n",
        "dev_data = load_data('dev.json')\n",
        "test_data = load_data('test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2mM6Cd6wvRM9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mM6Cd6wvRM9",
        "outputId": "5c359282-26c5-402b-f661-56fcb14fdf74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu gốc   : slide giáo trình đầy đủ .\n",
            "Đã tách từ: slide giáo_trình đầy_đủ .\n",
            "\n",
            "Câu gốc   : nhiệt tình giảng dạy , gần gũi với sinh viên .\n",
            "Đã tách từ: nhiệt_tình giảng_dạy , gần_gũi với sinh_viên .\n",
            "\n",
            "Câu gốc   : đi học đầy đủ full điểm chuyên cần .\n",
            "Đã tách từ: đi học đầy_đủ full điểm chuyên_cần .\n",
            "\n",
            "Câu gốc   : chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .\n",
            "Đã tách từ: chưa áp_dụng công_nghệ_thông_tin và các thiết_bị hỗ_trợ cho việc giảng_dạy .\n",
            "\n",
            "Câu gốc   : thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .\n",
            "Đã tách từ: thầy giảng bài hay , có nhiều bài_tập ví_dụ ngay trên lớp .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "  def segment_text_field(data_list, vncorenlp_model):\n",
        "      for idx, item in enumerate(data_list):\n",
        "          raw_text = item.get('sentence', '')\n",
        "          segmented_words = []\n",
        "          try:\n",
        "              if not raw_text.strip():\n",
        "                  raise ValueError(\"Câu rỗng hoặc chỉ chứa khoảng trắng.\")\n",
        "              annotated = vncorenlp_model.annotate_text(raw_text)\n",
        "              if not isinstance(annotated, dict) or 0 not in annotated:\n",
        "                  raise ValueError(f\"Kết quả không chứa khóa 0: {annotated}\")\n",
        "              for word_dict in annotated[0]:\n",
        "                  segmented_words.append(word_dict['wordForm'])\n",
        "              item['segmented_text'] = \" \".join(segmented_words)\n",
        "          except Exception as e:\n",
        "              print(f\"[LỖI tại index {idx}] {e}\\n→ Câu: '{raw_text}'\")\n",
        "              item['segmented_text'] = raw_text\n",
        "      return data_list\n",
        "\n",
        "  train_data = segment_text_field(train_data, model_vncorenlp)\n",
        "  dev_data = segment_text_field(dev_data, model_vncorenlp)\n",
        "  test_data = segment_text_field(test_data, model_vncorenlp)\n",
        "\n",
        "  for i in range(5):\n",
        "      print(f\"Câu gốc   : {train_data[i]['sentence']}\")\n",
        "      print(f\"Đã tách từ: {train_data[i]['segmented_text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CfAipsD_OWXT",
      "metadata": {
        "id": "CfAipsD_OWXT"
      },
      "source": [
        "## Bài 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "iaM9PdVKvRM9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iaM9PdVKvRM9",
        "outputId": "a48386ee-86a3-4ec9-ef26-8e1b46c69936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "eUXh-0i0G8Ou",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUXh-0i0G8Ou",
        "outputId": "cf54fc35-78ce-4b0d-88c7-828038a133f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước:\n",
            "Train: (11426, 768)\n",
            "Dev:   (1583, 768)\n",
            "Test:  (3166, 768)\n"
          ]
        }
      ],
      "source": [
        "  import torch\n",
        "  import numpy as np\n",
        "  import json\n",
        "  from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "  train_texts = [item['segmented_text'] for item in train_data if item.get('segmented_text')]\n",
        "  dev_texts   = [item['segmented_text'] for item in dev_data if item.get('segmented_text')]\n",
        "  test_texts  = [item['segmented_text'] for item in test_data if item.get('segmented_text')]\n",
        "\n",
        "  phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "  def get_phobert_embeddings(sentences, tokenizer, model, max_length=256, batch_size=64):\n",
        "      embeddings = []\n",
        "      model.eval()\n",
        "      for i in range(0, len(sentences), batch_size):\n",
        "          batch_texts = sentences[i:i + batch_size]\n",
        "          if not batch_texts or all(not t.strip() for t in batch_texts):\n",
        "              continue\n",
        "          inputs = tokenizer(batch_texts, return_tensors='pt', max_length=max_length,\n",
        "                            padding=True, truncation=True)\n",
        "          with torch.no_grad():\n",
        "              outputs = model(**inputs)\n",
        "              cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "              embeddings.append(cls_embeddings)\n",
        "      return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "  train_embeddings = get_phobert_embeddings(train_texts, tokenizer, phobert)\n",
        "  dev_embeddings   = get_phobert_embeddings(dev_texts, tokenizer, phobert)\n",
        "  test_embeddings  = get_phobert_embeddings(test_texts, tokenizer, phobert)\n",
        "\n",
        "  np.save('/content/uit_vsfc_data/train_embeddings.npy', train_embeddings)\n",
        "  np.save('/content/uit_vsfc_data/dev_embeddings.npy', dev_embeddings)\n",
        "  np.save('/content/uit_vsfc_data/test_embeddings.npy', test_embeddings)\n",
        "\n",
        "  print(f\"Kích thước:\")\n",
        "  print(\"Train:\", train_embeddings.shape)\n",
        "  print(\"Dev:  \", dev_embeddings.shape)\n",
        "  print(\"Test: \", test_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IzqLX22jOY_6",
      "metadata": {
        "id": "IzqLX22jOY_6"
      },
      "source": [
        "## Bài 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "FcxQtKBJmzv0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcxQtKBJmzv0",
        "outputId": "d31b7d02-1723-45b0-e376-ba670700fdd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision (Dev): [np.float64(0.49890590809409674), np.float64(0.0), np.float64(0.0)]\n",
            "Recall (Dev): [np.float64(0.9999999999912281), np.float64(0.0), np.float64(0.0)]\n",
            "F1 Score (Dev): [np.float64(0.6656934262118516), np.float64(0.0), np.float64(0.0)]\n",
            "Mean F1 (Dev): 0.2219\n",
            "Precision (Test): [np.float64(0.5022109917861586), np.float64(0.0), np.float64(0.0)]\n",
            "Recall (Test): [np.float64(0.9999999999937108), np.float64(0.0), np.float64(0.0)]\n",
            "F1 Score (Test): [np.float64(0.6686290956303261), np.float64(0.0), np.float64(0.0)]\n",
            "Mean F1 (Test): 0.2229\n"
          ]
        }
      ],
      "source": [
        "  import numpy as np\n",
        "  import json\n",
        "  from itertools import permutations\n",
        "\n",
        "  with open('/content/uit_vsfc_data/train.json', 'r', encoding='utf-8') as f:\n",
        "      train_data = json.load(f)\n",
        "  with open('/content/uit_vsfc_data/test.json', 'r', encoding='utf-8') as f:\n",
        "      test_data = json.load(f)\n",
        "\n",
        "  train_embeddings = np.load('/content/uit_vsfc_data/train_embeddings.npy')\n",
        "  test_embeddings = np.load('/content/uit_vsfc_data/test_embeddings.npy')\n",
        "\n",
        "  np.random.seed(42)\n",
        "  n_dev = int(0.2 * len(train_data))\n",
        "  indices = np.random.permutation(len(train_data))\n",
        "  dev_indices = indices[:n_dev]\n",
        "  train_indices = indices[n_dev:]\n",
        "\n",
        "  dev_data = [train_data[i] for i in dev_indices]\n",
        "  train_data_new = [train_data[i] for i in train_indices]\n",
        "  dev_embeddings = train_embeddings[dev_indices]\n",
        "  train_embeddings_new = train_embeddings[train_indices]\n",
        "\n",
        "  def standardize_data(data):\n",
        "      mean = np.mean(data, axis=0)\n",
        "      std = np.std(data, axis=0) + 1e-6\n",
        "      return (data - mean) / std\n",
        "\n",
        "  train_embeddings_new = standardize_data(train_embeddings_new)\n",
        "  dev_embeddings = standardize_data(dev_embeddings)\n",
        "  test_embeddings = standardize_data(test_embeddings)\n",
        "\n",
        "  def extract_labels(data):\n",
        "      label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "      return np.array([label_map[item['sentiment'].lower()] for item in data])\n",
        "\n",
        "  train_labels_new = extract_labels(train_data_new)\n",
        "  dev_labels = extract_labels(dev_data)\n",
        "  test_labels = extract_labels(test_data)\n",
        "\n",
        "  def gmm_em(data, n_clusters=3, n_iterations=100, tol=1e-4):\n",
        "      n_samples, n_features = data.shape\n",
        "      np.random.seed(42)\n",
        "      means = data[np.random.choice(n_samples, n_clusters, replace=False)]\n",
        "      covariances = np.array([np.cov(data.T) + np.eye(n_features) * 1e-6 for _ in range(n_clusters)])\n",
        "      weights = np.ones(n_clusters) / n_clusters\n",
        "      responsibilities = np.zeros((n_samples, n_clusters))\n",
        "      log_2pi = n_features * np.log(2 * np.pi)\n",
        "\n",
        "      for _ in range(n_iterations):\n",
        "          for k in range(n_clusters):\n",
        "              diff = data - means[k]\n",
        "              inv_cov = np.linalg.inv(covariances[k])\n",
        "              log_det = np.log(np.linalg.det(covariances[k]) + 1e-6)\n",
        "              exponent = -0.5 * np.sum(diff @ inv_cov * diff, axis=1)\n",
        "              log_prob = np.log(weights[k] + 1e-6) - 0.5 * (log_2pi + log_det) + exponent\n",
        "              responsibilities[:, k] = log_prob\n",
        "\n",
        "          max_log_prob = np.max(responsibilities, axis=1, keepdims=True)\n",
        "          responsibilities = np.exp(responsibilities - max_log_prob)\n",
        "          responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)\n",
        "\n",
        "          Nk = np.sum(responsibilities, axis=0)\n",
        "          means_new = (responsibilities.T @ data) / Nk[:, None]\n",
        "\n",
        "          for k in range(n_clusters):\n",
        "              diff = data - means_new[k]\n",
        "              covariances[k] = (responsibilities[:, k][:, None] * diff).T @ diff / Nk[k]\n",
        "              covariances[k] += np.eye(n_features) * 1e-6\n",
        "\n",
        "          weights_new = Nk / n_samples\n",
        "          if np.all(np.abs(means_new - means) < tol):\n",
        "              break\n",
        "\n",
        "          means, weights = means_new, weights_new\n",
        "\n",
        "      return means, covariances, weights\n",
        "\n",
        "  def predict_labels(data, means, covariances, weights):\n",
        "      n_samples, n_features = data.shape\n",
        "      n_clusters = len(means)\n",
        "      responsibilities = np.zeros((n_samples, n_clusters))\n",
        "      log_2pi = n_features * np.log(2 * np.pi)\n",
        "\n",
        "      for k in range(n_clusters):\n",
        "          diff = data - means[k]\n",
        "          inv_cov = np.linalg.inv(covariances[k])\n",
        "          log_det = np.log(np.linalg.det(covariances[k]) + 1e-6)\n",
        "          exponent = -0.5 * np.sum(diff @ inv_cov * diff, axis=1)\n",
        "          log_prob = np.log(weights[k] + 1e-6) - 0.5 * (log_2pi + log_det) + exponent\n",
        "          responsibilities[:, k] = log_prob\n",
        "\n",
        "      max_log_prob = np.max(responsibilities, axis=1, keepdims=True)\n",
        "      responsibilities = np.exp(responsibilities - max_log_prob)\n",
        "      return np.argmax(responsibilities, axis=1)\n",
        "\n",
        "  def compute_metrics(true_labels, pred_labels):\n",
        "      n_classes = len(set(true_labels))\n",
        "      best_f1 = 0\n",
        "      best_perm = None\n",
        "\n",
        "      for perm in permutations(range(n_classes)):\n",
        "          mapped = np.array([perm[p] for p in pred_labels])\n",
        "          precision, recall, f1 = [], [], []\n",
        "\n",
        "          for c in range(n_classes):\n",
        "              tp = np.sum((mapped == c) & (true_labels == c))\n",
        "              fp = np.sum((mapped == c) & (true_labels != c))\n",
        "              fn = np.sum((mapped != c) & (true_labels == c))\n",
        "\n",
        "              p = tp / (tp + fp + 1e-8)\n",
        "              r = tp / (tp + fn + 1e-8)\n",
        "              f = 2 * p * r / (p + r + 1e-8)\n",
        "\n",
        "              precision.append(p)\n",
        "              recall.append(r)\n",
        "              f1.append(f)\n",
        "\n",
        "          if np.mean(f1) > best_f1:\n",
        "              best_f1 = np.mean(f1)\n",
        "              best_result = (precision, recall, f1, perm)\n",
        "\n",
        "      return best_result\n",
        "\n",
        "  means, covariances, weights = gmm_em(train_embeddings_new, n_clusters=3)\n",
        "\n",
        "  dev_preds = predict_labels(dev_embeddings, means, covariances, weights)\n",
        "  precision_dev, recall_dev, f1_dev, best_perm_dev = compute_metrics(dev_labels, dev_preds)\n",
        "  print(\"Precision (Dev):\", precision_dev)\n",
        "  print(\"Recall (Dev):\", recall_dev)\n",
        "  print(\"F1 Score (Dev):\", f1_dev)\n",
        "  print(f\"Mean F1 (Dev): {np.mean(f1_dev):.4f}\")\n",
        "\n",
        "  test_preds = predict_labels(test_embeddings, means, covariances, weights)\n",
        "  precision_test, recall_test, f1_test, best_perm_test = compute_metrics(test_labels, test_preds)\n",
        "  print(\"Precision (Test):\", precision_test)\n",
        "  print(\"Recall (Test):\", recall_test)\n",
        "  print(\"F1 Score (Test):\", f1_test)\n",
        "  print(f\"Mean F1 (Test): {np.mean(f1_test):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}